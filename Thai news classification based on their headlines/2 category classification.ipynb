{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thai news classification based on their headlines\n",
    "# AI จัดหมวดหมู่ข่าวไทยโดยใช้การพาดหัวข่าว\n",
    "## by Wisarut Duangmorakot - wisarut.bank@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data ได้มาจากการเขียน Script ดึงข้อมูลจากเว็บไซต์ข่าวสำนักนึงของไทย ทำให้ไม่สามารถเปิดเผยข้อมูลได้\n",
    "\n",
    "- เราจะทำการจัดหมวดหมู่ข่าวโดยใช้การพาดหัวข่าว โดยจัดเป็น 2 หมวดหมู่คือ \"ข่าวการเมือง\" และ \"ข่าวกีฬา\" เนื่องจากข้อมูลมีขนาดใหญ่และ Computation มีจำกัด ทำให้เกิด Memory Error ขึ้นได้\n",
    "\n",
    "- ใช้ pythainlp ในการช่วยตัดคำภาษาไทย"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os #ใช้ในการเปิดไฟล์ทั้งหมดใน Directory ขึ้นมาอ่าน\n",
    "from pythainlp import word_tokenize #ใช้ตัดคำภาษาไทย เช่น ฉันกำลังขึ้นรถไฟ -> 'ฉัน','กำลัง','ขึ้น','รถไฟ'\n",
    "from sklearn.metrics import accuracy_score # ใช้เพื่อเพื่อวัดประสิทธิความแม่นยำของโมเดล\n",
    "from sklearn.metrics import confusion_matrix # ใช้เพื่อเพื่อวัดประสิทธิภาพโมเดล\n",
    "import re #ใช้ในการกำจัดตัวอักษรหรือคำที่ไม่ต้องการออกไป"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60568\n",
      "67790\n"
     ]
    }
   ],
   "source": [
    "news = []\n",
    "politics = 0\n",
    "sports = 0\n",
    "\n",
    "path = './politics/'\n",
    "for filename in os.listdir(path): # เปิดไฟล์ทุกไฟล์ในโฟลเดอร์ politics\n",
    "    f = open(path+filename,'r') # เปิดไฟล์ด้วยโหมดอ่าน\n",
    "    text = f.read()\n",
    "    text = text.replace(' ','') # ลบเว้นวรรคออกจากพาดหัวข่าว\n",
    "    text = text.replace('\"','')\n",
    "    news.append(text)\n",
    "    f.close()\n",
    "    politics += 1\n",
    " \n",
    "print (politics)    \n",
    "    \n",
    "path = './sports/'\n",
    "for filename in os.listdir(path):\n",
    "    f = open(path+filename,'r') \n",
    "    text = f.read()\n",
    "    text = text.replace(' ','')\n",
    "    text = text.replace('\"','')\n",
    "    news.append(text)\n",
    "    f.close()\n",
    "    sports += 1\n",
    "\n",
    "print (sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Datasets เข้ามาเก็บไว้ในตัวแปร news\n",
    "ซึ่ง Dataset ประกอบไปด้วยพาดหัวข่าวการเมือง 60568 samples ข่าวกีฬา 67790 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ตัวอย่างการพาดหัวข่าว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ไทเกอร์หายไม่ทันหวดยูเอสโอเพ่น\n"
     ]
    }
   ],
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\-)|(0)|(1)|(2)|(3)|(4)|(5)|(6)|(7)|(8)|(9)|(&quot;)|(\\”)|(\\\")|(\\“)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "news_pre = preprocess_reviews(news) # เอาสัญลักษณ์ที่ไม่เกี่ยวข้องในการ analysis ออก\n",
    "\n",
    "\n",
    "train = []\n",
    "\n",
    "for i in range(len(news_pre)):\n",
    "    proc = word_tokenize(news[i], engine='newmm') #ใช้ตัดคำภาษาไทย เช่น ฉันกำลังขึ้นรถไฟ -> 'ฉัน','กำลัง','ขึ้น','รถไฟ'\n",
    "    train.append(proc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ตัวอย่าง text หลังจากผ่านการทำ Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ไทเกอร์', 'หาย', 'ไม่ทัน', 'หวด', 'ยู', 'เอส', 'โอเพ่น']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def do_nothing(tokens):\n",
    "    return tokens # เราไม่ใช้ tokenizer ใดๆในการตัดคำ เพราะใช้ pythainlp word_tokeniz แทนแล้ว\n",
    "\n",
    "\n",
    "cv = CountVectorizer(tokenizer=do_nothing, preprocessor=None, lowercase=False)\n",
    "X = cv.fit_transform(train) # apply Countvertorizer to data\n",
    "\n",
    "y = []\n",
    "\n",
    "for i in range(politics):\n",
    "    y.append(1) #label ข่าวการเมืองด้วย 1\n",
    "    \n",
    "for i in range(sports):\n",
    "    y.append(0) #label ข่าวกีฬาด้วย 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราไม่สามารถเอา text เข้า model classification ได้ทันที จะต้องเปลี่ยนจาก text แทนด้วยการนับจำนวนคำแต่ละคำ เช่น\n",
    "\n",
    "corpus = [<br>\n",
    "...     'This is the first document.',<br>\n",
    "...     'This document is the second document.',<br>\n",
    "...     'And this is the third one.',<br>\n",
    "...     'Is this the first document?',<br>\n",
    "... ]<br>\n",
    "\n",
    "vectorizer = CountVectorizer()<br>\n",
    "X = vectorizer.fit_transform(corpus)<br>\n",
    "print(vectorizer.get_feature_names())<br>\n",
    "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']<br>\n",
    "print(X.toarray())  <br><br>\n",
    "[[0 1 1 1 0 0 1 0 1]<br>\n",
    " [0 2 0 1 0 1 1 0 1]<br>\n",
    " [1 0 0 1 1 0 1 1 1]<br>\n",
    " [0 1 1 1 0 0 1 0 1]]<br>\n",
    " \n",
    " http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128358"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train) # dataset ทั้งหมด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ใช้ train_test_split เพื่อ random train dataset เพื่อใช้ในการเทรนโมเดล และ test dataset เพื่อใช้ในการวัดผลโมเดล"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=4,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 1000, n_jobs=4)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ใช้โมเดลมาตรฐานในการสร้างโมเดล เพื่อความรวดเร็ว ถ้านำไปใช้จริงอาจจะต้อง tune parameters ด้วย gridsearch และอาจใช้ voting method เพื่อเพิ่มประสิทธิภาพของโมเดลให้แม่นยำขึ้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9761997507011531"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13181,   355],\n",
       "       [  256, 11880]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "โมเดลมีความแม่นยำถึง 97.62% และไม่เกิดการ overfitted ที่คลาสใดคลาสหนึ่ง"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทดสอบการนำโมเดลไปใช้จริง โดยลองเทสกับข่าวที่อยู่คนละสำนักพิมพ์กับที่นำข้อมูลมาเทรนและเป็นข้อมูลที่โมเดลไม่เคยเห็นมาก่อน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['บิ๊ก', 'ตู่', 'นิ้วมือ', 'ซ้น', 'แต่', 'ยัง', 'ฟิต', 'ลุย', 'ถก', 'ครม.', 'ต่อ', 'หลัง', 'บิน', 'กลับ', 'จาก', 'ญี่ปุ่น']\n",
      "['ครม.', 'โยก', 'สม', 'ชัย', 'จาก', 'ปลัด', 'คลัง', 'นั่ง', 'เลขาฯ', 'สภาพัฒน์']\n",
      "['กำลัง', 'ใจมา', 'เต็ม', 'โอ๊ค', 'ยัน', 'ไม่', 'หนี', 'คดี', 'ฟอกเงิน', 'โพสต์', 'เฟซฯ', 'เย้ย', 'เผด็จการ']\n",
      "['สื่อ', 'อังกฤษ', 'แฉ', 'แข้ง', 'แมนฯยู', 'ไม่พอใจ', 'โดน', 'เปลี่ยนตัว', 'เกม', 'ทุบ', 'นิวคาสเซิล']\n",
      "['กุนซือ', 'ดัตช์', 'ตำหนิ', 'ฟาน', 'ได', 'ค์', 'หลัง', 'เริ่ม', 'ฟอร์ม', 'หลุด']\n",
      "['ซี', 'ดาน', 'เล็ง', 'คว้า', 'แข้ง', 'จอม', 'บุก', 'บาร์', 'ซา', 'หาก', 'ได้', 'คุม', 'แมนฯยู']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ['บิ๊กตู่ นิ้วมือซ้น แต่ยังฟิต ลุยถก ครม.ต่อ หลังบินกลับจากญี่ปุ่น',\\\n",
    "     'ครม. โยก สมชัย จากปลัดคลัง นั่งเลขาฯ สภาพัฒน์'\\\n",
    "     ,'กำลังใจมาเต็ม โอ๊ค ยันไม่หนีคดีฟอกเงิน โพสต์เฟซฯ เย้ยเผด็จการ'\\\n",
    "     ,'สื่ออังกฤษแฉ! 2 แข้งแมนฯยูไม่พอใจ โดนเปลี่ยนตัวเกมทุบนิวคาสเซิล'\\\n",
    "    ,'กุนซือดัตช์ตำหนิ ฟาน ไดค์ หลังเริ่มฟอร์มหลุด'\\\n",
    "     ,'ซีดาน เล็งคว้าแข้งจอมบุกบาร์ซา หากได้คุมแมนฯยู']\n",
    "\n",
    "for i in range(len(s)):\n",
    "    s[i] = s[i].replace(' ','')\n",
    "\n",
    "\n",
    "news_pre = preprocess_reviews(s) \n",
    "\n",
    "news_sample = []\n",
    "\n",
    "for i in range(len(s)):\n",
    "    proc = word_tokenize(news_pre[i], engine='newmm') #ใช้ตัดคำภาษาไทย เช่น ฉันกำลังขึ้นรถไฟ -> 'ฉัน','กำลัง','ขึ้น','รถไฟ'\n",
    "    print (proc)\n",
    "    news_sample.append(proc)\n",
    "\n",
    "news_sample = cv.transform(news_sample)\n",
    "clf.predict(news_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "โมเดลสามารถทำงานได้อย่างถูกต้อง (0 แทนข่าวกีฬา, 1 แทนข่าวการเมือง)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- สามารถสร้างโมเดลที่ทำนายประเภทของข่าวได้โดยใช้การพาดหัว ซึ่งมีความแม่นยำ 97.62%\n",
    "- สามารถเพิ่มประสิทธิภาพของโมเดลได้ โดยการ tune model เช่นใช้ Gridsearch หรือ การ voting ของโมเดล\n",
    "- การไม่นำข้อมูลที่เกิดขึ้นบ่อยเกินหรือน้อยเกินมาสร้างโมเดลก็ทำให้โมเดลดีขึ้นได้เช่นกัน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.decode\n",
    "- https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184\n",
    "- https://stackoverflow.com/questions/18262293/how-to-open-every-file-in-a-folder\n",
    "- https://www.pythonforbeginners.com/files/reading-and-writing-files-in-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
